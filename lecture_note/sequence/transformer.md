[<-PREV](sequence.md)

# Transformer 

## 1. Intuition
![image](images/image1.png)

## 2. Attention mechanism and Transformer's architecture

![image](images/equation1.png)
![image](images/image2_2.png)

### 3. Pre-training Transformer models
![image](images/image3.png)
- Many more options at [https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)


## 4. BERT (Bidirectional Encoder Representations from Transformers)
### 4.1 Text preprocessing

### 4.2 Pretraining
1. Semi-supervised training on larget amounts of text
  - Masked Language Model (MLM)
  - Next Sentence Prediction (NSP) 

2. Supervised training on a specific task with a labeled dataset 


[<-PREV](sequence.md)
