[<-PREV](sequence.md)

# Transformer 

## 1. Intuition
![image](images/image1.png)

## 2. Attention mechanism and Transformer's architecture

![image](images/equation1.png)
![image](images/image2_2.png)

## 3. BERT (Bidirectional Encoder Representations from Transformers)
### 3.1 BERT's architecture
- BERT Base: 12 layers, 12 attention heads, and 110 million parameters
- BERT Large: 24 layers, 16 attention heads and, 340 million parameters

  https://huggingface.co/transformers/pretrained_models.html



[<-PREV](sequence.md)
